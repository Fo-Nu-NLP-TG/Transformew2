# Recommended References

This folder is for storing reference papers and resources for studying transformer models. Below are some recommended papers to include in this folder:

## Essential Papers

1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - The original transformer paper
   - URL: https://arxiv.org/abs/1706.03762

2. **"On Layer Normalization in the Transformer Architecture"** (Xiong et al., 2020)
   - Explains the importance of layer normalization
   - URL: https://arxiv.org/abs/2002.04745

3. **"Understanding the Difficulty of Training Transformers"** (Liu et al., 2020)
   - Insights on initialization and training stability
   - URL: https://arxiv.org/abs/2004.08249

4. **"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** (Raffel et al., 2020)
   - T5 paper with insights on transformer architecture
   - URL: https://arxiv.org/abs/1910.10683

5. **"Neural Machine Translation of Rare Words with Subword Units"** (Sennrich et al., 2016)
   - BPE tokenization, important for understanding the tokenization process
   - URL: https://arxiv.org/abs/1508.07909

## Low-Resource Translation Papers

1. **"Massively Multilingual Neural Machine Translation in the Wild"** (Arivazhagan et al., 2019)
   - Insights on multilingual translation including low-resource languages
   - URL: https://arxiv.org/abs/1907.05019

2. **"Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation"** (Zhang et al., 2020)
   - Techniques for improving low-resource translation
   - URL: https://arxiv.org/abs/2004.11867

3. **"Unsupervised Neural Machine Translation with Weight Sharing"** (Yang et al., 2018)
   - Techniques for scenarios with limited parallel data
   - URL: https://arxiv.org/abs/1804.09057

## Optimization and Training

1. **"Adam: A Method for Stochastic Optimization"** (Kingma & Ba, 2015)
   - The Adam optimizer used in transformer training
   - URL: https://arxiv.org/abs/1412.6980

2. **"Rethinking the Inception Architecture for Computer Vision"** (Szegedy et al., 2016)
   - Introduces label smoothing, used in transformer training
   - URL: https://arxiv.org/abs/1512.00567

3. **"Dropout: A Simple Way to Prevent Neural Networks from Overfitting"** (Srivastava et al., 2014)
   - Explains dropout regularization used in transformers
   - URL: https://jmlr.org/papers/v15/srivastava14a.html

## How to Use These References

1. Download the PDF files of these papers
2. Place them in this folder
3. Read them alongside studying the TransformEw2 code
4. Take notes on how the concepts in the papers are implemented in the code

These references will provide the theoretical foundation for understanding the TransformEw2 implementation and its improvements over TransformEw1.
